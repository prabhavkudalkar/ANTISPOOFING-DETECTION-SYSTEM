Automated manipulation of videos and images has significantly advanced in recent years. In particular, the creation of forged videos involving swapped faces, known as deep fakes, has garnered considerable attention for their potential use in fraud, defamation, entertainment, and the dissemination of misinformation. The sophistication of current techniques has reached a level where distinguishing between real and fake video content featuring human faces has become increasingly challenging for humans. Consequently, there is growing concern about the implications of this technology(RÃ¶ssler et al., 2019).
Artificial intelligence-assisted videos and images that have been modified to appear different from their original form are called "deepfakes." This technology was first developed primarily for the entertainment industry to produce synthetic videos (www.youtube.com, n.d.). Later, it was applied to produce lifelike visuals for influential science fiction films and animated features. As the field developed, programs that let users make humorous use of face-swapping features appeared. Although these programmes were easily available, they weren't always reliable sources of information. But when deep learning was added to this technology, a big breakthrough happened. Artificial intelligence helped deepfakes become the most realistically altered images and videos on the internet.However, as machine learning technology develops, security and data protection issues often arise that require attention. There is an urgent need to confirm the legitimacy of these videos due to the notable rise in websites that upload and distribute videos. When uncertainty hits, it's critical to come up with a variety of creative solutions to deal with the problems at hand. The creation of deepfake algorithms that produce distorted information has advanced recently, and this has had negative effects on mass communication, anonymity, and security.
The primary aim of these detection methods is to mitigate the risks associated with deep fakes. However, due to the rapid advancements in this field, continuous evaluation of new detection approaches is essential to address this ongoing challenge effectively. It presents a model that protects against GAN-based Deepfake attacks by leveraging the knowledge of adversarial faces(Yang et al., 2021). To conceal people's faces, the model employs an adversarial face generation technique that takes into account arbitrary picture transformations when building the Deepfake model. This method will inevitably result in more artefacts in synthesised expressions, making it much more difficult to identify the fake images and videos that are created. Modifications in adversarial and edge losses are important initiators of a significant decline in the synthesised images' content. The robustness and dependability of the model with a specific metrics focus are demonstrated in the paper
